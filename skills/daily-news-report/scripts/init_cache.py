#!/usr/bin/env python3
"""
Daily News Report ç¼“å­˜è¿ç§»è„šæœ¬
å°†æ—§ç‰ˆ cache.json è¿ç§»åˆ°æ–°çš„æ—¥å¿—ç³»ç»Ÿ
"""

import json
import os
from datetime import datetime
from pathlib import Path

# é…ç½®
CACHE_DIR = Path(__file__).parent.parent / "cache"
OLD_CACHE_FILE = CACHE_DIR.parent / "cache.json"
INDEX_FILE = CACHE_DIR / "index.json"

def load_old_cache():
    """åŠ è½½æ—§ç‰ˆç¼“å­˜æ•°æ®"""
    if not OLD_CACHE_FILE.exists():
        print(f"âš ï¸  æ—§ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {OLD_CACHE_FILE}")
        return None
    
    with open(OLD_CACHE_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_daily_log_for_today():
    """ä¸ºä»Šå¤©åˆ›å»ºæ—¥å¿—"""
    date_str = datetime.now().strftime("%Y-%m-%d")
    run_id = f"run_{date_str}_001"
    
    # åˆ›å»ºæ—¥å¿—æ•°æ®ç»“æ„
    log_data = {
        "schema_version": "2.0",
        "description": "Daily News Report æ¯æ—¥è¿è¡Œæ—¥å¿—",
        
        "run_info": {
            "date": date_str,
            "run_id": run_id,
            "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
            "duration_seconds": 0,
            "phase": "initializing"
        },
        
        "summary": {
            "sources_fetched": 0,
            "items_collected": 0,
            "items_published": 0,
            "items_deduped": 0,
            "quality_avg": 0.0,
            "status": "pending"
        },
        
        "sources": {
            "hn": {"status": "pending", "items_collected": 0, "avg_quality": 0.0},
            "hf_papers": {"status": "pending", "items_collected": 0, "avg_quality": 0.0},
            "producthunt": {"status": "pending", "items_collected": 0, "avg_quality": 0.0},
            "hackernoon_pm": {"status": "pending", "items_collected": 0, "avg_quality": 0.0},
            "jamesclear": {"status": "pending", "items_collected": 0, "avg_quality": 0.0},
            "fs_blog": {"status": "pending", "items_collected": 0, "avg_quality": 0.0},
            "scottyoung": {"status": "pending", "items_collected": 0, "avg_quality": 0.0},
            "stripe_blog": {"status": "pending", "items_collected": 0, "avg_quality": 0.0},
            "paulgraham": {"status": "pending", "items_collected": 0, "avg_quality": 0.0},
            "dmitrybrant": {"status": "pending", "items_collected": 0, "avg_quality": 0.0}
        },
        
        "url_cache": {
            "comment": "æœ¬æ—¥æŠ“å–çš„æ‰€æœ‰URLï¼Œç”¨äºå»é‡",
            "urls": []
        },
        
        "content_hashes": {
            "comment": "å†…å®¹æŒ‡çº¹ï¼Œç”¨äºè·¨æ—¥å»é‡",
            "hashes": {}
        },
        
        "articles": {
            "comment": "æœ¬æ—¥æ”¶å½•çš„æ‰€æœ‰æ–‡ç« æ•°æ®",
            "items": []
        },
        
        "errors": {
            "comment": "è¿è¡Œè¿‡ç¨‹ä¸­çš„æ‰€æœ‰é”™è¯¯ä¿¡æ¯",
            "items": []
        },
        
        "metadata": {
            "environment": "local",
            "agent_version": "3.0",
            "worker_count": 1,
            "parallel_enabled": False
        }
    }
    
    return log_data

def update_index_file():
    """æ›´æ–°ç´¢å¼•æ–‡ä»¶"""
    # åˆ›å»ºæˆ–è¯»å–ç´¢å¼•
    if INDEX_FILE.exists():
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
    else:
        index_data = {
            "schema_version": "2.0",
            "description": "Daily News Report ç¼“å­˜ç³»ç»Ÿç´¢å¼•æ–‡ä»¶",
            "last_updated": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
            "total_runs": 0,
            "available_dates": []
        }
    
    # æ›´æ–°å¯ç”¨æ—¥æœŸ
    date_str = datetime.now().strftime("%Y-%m-%d")
    available_dates = set(index_data.get("available_dates", []))
    available_dates.add(date_str)
    index_data["available_dates"] = sorted(list(available_dates), reverse=True)
    index_data["total_runs"] = index_data.get("total_runs", 0) + 1
    index_data["last_updated"] = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    
    # ä¿å­˜ç´¢å¼•
    with open(INDEX_FILE, 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç´¢å¼•æ–‡ä»¶å·²æ›´æ–°: {len(index_data['available_dates'])} ä¸ªå¯ç”¨æ—¥æœŸ")

def main():
    """ä¸»è¿ç§»æµç¨‹"""
    print("=" * 60)
    print("ğŸ”„ Daily News Report ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–")
    print("=" * 60)
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = CACHE_DIR / "logs"
    log_dir.mkdir(exist_ok=True)
    print(f"ğŸ“ åˆ›å»ºæ—¥å¿—ç›®å½•: {log_dir}")
    
    # ä¸ºä»Šå¤©åˆ›å»ºæ—¥å¿—
    date_str = datetime.now().strftime("%Y-%m-%d")
    log_data = create_daily_log_for_today()
    log_file = log_dir / f"{date_str}.json"
    
    # ä¿å­˜æ—¥å¿—
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… åˆ›å»ºä»Šæ—¥æ—¥å¿—: {log_file}")
    
    # æ›´æ–°ç´¢å¼•æ–‡ä»¶
    update_index_file()
    
    print("\n" + "=" * 60)
    print(f"âœ… ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    print(f"   - ç´¢å¼•æ–‡ä»¶: {INDEX_FILE}")
    print(f"   - æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"   - ä»Šæ—¥æ—¥å¿—: {log_file}")
    print("=" * 60)

if __name__ == "__main__":
    main()
