#!/usr/bin/env python3
"""
Daily News Report ç¼“å­˜è¿ç§»è„šæœ¬
å°†æ—§ç‰ˆ cache.json è¿ç§»åˆ°æ–°çš„æ—¥å¿—ç³»ç»Ÿ
"""

import json
import os
from datetime import datetime
from pathlib import Path

# é…ç½®
CACHE_DIR = Path(__file__).parent.parent / "cache"
OLD_CACHE_FILE = CACHE_DIR.parent / "cache.json"
INDEX_FILE = CACHE_DIR / "index.json"

def load_old_cache():
    """åŠ è½½æ—§ç‰ˆç¼“å­˜æ•°æ®"""
    if not OLD_CACHE_FILE.exists():
        print(f"âš ï¸  æ—§ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {OLD_CACHE_FILE}")
        return None
    
    with open(OLD_CACHE_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_daily_log(old_data, date_str):
    """ä»æ—§æ•°æ®åˆ›å»ºæ¯æ—¥æ—¥å¿—"""
    
    # ç”Ÿæˆrun_id
    run_id = f"run_{date_str}_migrated_001"
    
    # åˆ›å»ºæ—¥å¿—æ•°æ®ç»“æ„
    log_data = {
        "schema_version": "2.0",
        "description": "Daily News Report æ¯æ—¥è¿è¡Œæ—¥å¿— - è¿ç§»ç‰ˆæœ¬",
        
        "run_info": {
            "date": date_str,
            "run_id": run_id,
            "timestamp": f"{date_str}T12:00:00Z",
            "duration_seconds": old_data.get("last_run", {}).get("duration_seconds", 0),
            "phase": "done"
        },
        
        "summary": {
            "sources_fetched": len(old_data.get("sources_used", [])),
            "items_collected": old_data.get("last_run", {}).get("items_collected", 0),
            "items_published": old_data.get("last_run", {}).get("items_published", 0),
            "items_deduped": 0,  # æ—§æ•°æ®æ— æ­¤å­—æ®µ
            "quality_avg": calculate_avg_quality(old_data),
            "status": "migrated"
        },
        
        "sources": migrate_sources(old_data, date_str),
        "url_cache": migrate_urls(old_data, date_str),
        "content_hashes": migrate_hashes(old_data),
        "articles": migrate_articles(old_data, date_str),
        "errors": [],
        "metadata": {
            "environment": "local",
            "agent_version": "2.0",
            "worker_count": 1,
            "parallel_enabled": False
        }
    }
    
    return log_data

def calculate_avg_quality(old_data):
    """è®¡ç®—å¹³å‡è´¨é‡åˆ†"""
    source_stats = old_data.get("source_stats", {})
    total_quality = 0
    total_items = 0
    
    for source_id, stats in source_stats.items():
        avg_q = stats.get("avg_quality_score", 0)
        items = stats.get("avg_items_per_fetch", 0)
        total_quality += avg_q * items
        total_items += items
    
    return round(total_quality / total_items, 2) if total_items > 0 else 0.0

def migrate_sources(old_data, date_str):
    """è¿ç§»æºç»Ÿè®¡ä¿¡æ¯"""
    sources = {}
    
    # ä»article_historyæå–æ–‡ç« æ¥æº
    article_history = old_data.get("article_history", {})
    articles_by_source = {}
    
    for date, articles in article_history.items():
        for article in articles:
            # æ ¹æ®æ–‡ç« æ ‡é¢˜æ¨æ–­æ¥æº
            title = article.get("title", "")
            source_id = infer_source_from_title(title)
            if source_id not in articles_by_source:
                articles_by_source[source_id] = []
            articles_by_source[source_id].append(article)
    
    # æ„å»ºsourcesæ•°æ®
    source_stats = old_data.get("source_stats", {})
    
    for source_id in ["hn", "hf_papers", "producthunt", "hackernoon_pm", 
                   "jamesclear", "fs_blog", "scottyoung", "stripe_blog",
                   "paulgraham", "dmitrybrant"]:
        sources[source_id] = {
            "status": "migrated",
            "items_collected": len(articles_by_source.get(source_id, [])),
            "items_published": len(articles_by_source.get(source_id, [])),
            "avg_quality": source_stats.get(source_id, {}).get("avg_quality_score", 0.0),
            "duration_ms": 0,
            "error": None,
            "urls_fetched": []
        }
    
    return sources

def infer_source_from_title(title):
    """ä»æ–‡ç« æ ‡é¢˜æ¨æ–­æ¥æº"""
    title_lower = title.lower()
    
    if "xai" in title_lower or "spacex" in title_lower:
        return "spacex"
    elif "codex" in title_lower:
        return "openai"
    elif "anki" in title_lower:
        return "anki"
    elif "github" in title_lower or "sudo" in title_lower or "moltbook" in title_lower:
        return "github"
    elif "nano-vllm" in title_lower or "zig" in title_lower or "rclone" in title_lower:
        return "hackernews_tech"
    elif "ebisu" in title_lower or "adaptive" in title_lower or "fs-researcher" in title_lower:
        return "huggingface"
    elif "moltbook" in title_lower or "chaching" in title_lower or "amara" in title_lower:
        return "producthunt"
    elif "metronome" in title_lower or "stripe" in title_lower or "agentic" in title_lower:
        return "stripe_blog"
    elif "3-2-1" in title_lower:
        return "jamesclear"
    elif "independence" in title_lower or "energy" in title_lower:
        return "fs_blog"
    elif "stress" in title_lower or "energy" in title_lower or "learn taste" in title_lower:
        return "scottyoung"
    elif "alignment" in title_lower or "stakeholders" in title_lower or "broken" in title_lower or "ideaops" in title_lower:
        return "hackernoon_pm"
    elif "largest number" in title_lower or "game arena" in title_lower:
        return "google_deepmind"
    elif "how we went" in title_lower or "design review" in title_lower:
        return "hackernoon_pm"
    elif "businesses grow" in title_lower or "introducing" in title_lower:
        return "stripe_blog"
    else:
        return "unknown"

def migrate_urls(old_data, date_str):
    """è¿ç§»URLç¼“å­˜"""
    url_cache = old_data.get("url_cache", {}).get("entries", {})
    
    return {
        "comment": f"ä»æ—§ç³»ç»Ÿè¿ç§»çš„URLç¼“å­˜ - {date_str}",
        "urls": list(url_cache.keys())
    }

def migrate_hashes(old_data):
    """è¿ç§»å†…å®¹æŒ‡çº¹"""
    content_hashes = old_data.get("content_hashes", {}).get("entries", {})
    
    return {
        "comment": "ä»æ—§ç³»ç»Ÿè¿ç§»çš„å†…å®¹æŒ‡çº¹",
        "hashes": content_hashes
    }

def migrate_articles(old_data, date_str):
    """è¿ç§»æ–‡ç« æ•°æ®"""
    article_history = old_data.get("article_history", {})
    
    # è·å–æŒ‡å®šæ—¥æœŸçš„æ–‡ç« 
    articles = article_history.get(date_str, [])
    
    # ä¸ºæ¯ç¯‡æ–‡ç« æ·»åŠ å…ƒæ•°æ®
    articles_with_metadata = []
    for idx, title in enumerate(articles, 1):
        articles_with_metadata.append({
            "id": idx,
            "title": title,
            "source_id": infer_source_from_title(title),
            "url": f"https://example.com/article/{idx}",  # æ—§æ•°æ®æ— URL
            "summary": "Migrated from old cache",
            "key_points": [],
            "keywords": [],
            "quality_score": 4,
            "fetched_at": f"{date_str}T12:00:00Z"
        })
    
    return {
        "comment": f"ä»æ—§ç³»ç»Ÿè¿ç§»çš„æ–‡ç«  - {date_str}",
        "items": articles_with_metadata
    }

def update_index_file(old_data):
    """æ›´æ–°ç´¢å¼•æ–‡ä»¶"""
    # è¯»å–ç°æœ‰ç´¢å¼•æˆ–åˆ›å»ºæ–°çš„
    if INDEX_FILE.exists():
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
    else:
        index_data = {
            "schema_version": "2.0",
            "description": "Daily News Report ç¼“å­˜ç³»ç»Ÿç´¢å¼•æ–‡ä»¶",
            "last_updated": "2026-02-03T12:00:00Z",
            "total_runs": 0,
            "available_dates": []
        }
    
    # æ›´æ–°æ€»è¿è¡Œæ¬¡æ•°
    index_data["total_runs"] = index_data.get("total_runs", 0) + len(old_data.get("article_history", {}))
    
    # æ›´æ–°å¯ç”¨æ—¥æœŸåˆ—è¡¨
    available_dates = set(index_data.get("available_dates", []))
    for date in old_data.get("article_history", {}).keys():
        available_dates.add(date)
    index_data["available_dates"] = sorted(list(available_dates), reverse=True)
    
    # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
    index_data["last_updated"] = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    
    # ä¿å­˜ç´¢å¼•
    with open(INDEX_FILE, 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç´¢å¼•æ–‡ä»¶å·²æ›´æ–°: {len(index_data['available_dates'])} ä¸ªå¯ç”¨æ—¥æœŸ")

def main():
    """ä¸»è¿ç§»æµç¨‹"""
    print("=" * 60)
    print("ğŸ”„ Daily News Report ç¼“å­˜è¿ç§»å·¥å…·")
    print("=" * 60)
    
    # åŠ è½½æ—§ç¼“å­˜
    old_cache = load_old_cache()
    if old_cache is None:
        print("âŒ æ— æ³•è¿ç§»ï¼šæ—§ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    print(f"ğŸ“‚ åŠ è½½æ—§ç¼“å­˜: {OLD_CACHE_FILE}")
    
    # è·å–æ–‡ç« å†å²
    article_history = old_cache.get("article_history", {})
    print(f"ğŸ“Š å‘ç° {len(article_history)} å¤©çš„å†å²æ•°æ®")
    
    # è¿ç§»æ¯ä¸ªæ—¥æœŸ
    migrated_count = 0
    for date_str, articles in article_history.items():
        print(f"\nğŸ”„ è¿ç§»æ—¥æœŸ: {date_str} ({len(articles)} ç¯‡æ–‡ç« )")
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = CACHE_DIR / "logs"
        log_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ¯æ—¥æ—¥å¿—æ–‡ä»¶
        log_data = create_daily_log(old_cache, date_str)
        log_file = log_dir / f"{date_str}.json"
        
        # ä¿å­˜æ—¥å¿—
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ… åˆ›å»ºæ—¥å¿—: {log_file}")
        migrated_count += 1
    
    # æ›´æ–°ç´¢å¼•æ–‡ä»¶
    update_index_file(old_cache)
    
    print("\n" + "=" * 60)
    print(f"âœ… è¿ç§»å®Œæˆï¼")
    print(f"   - è¿ç§»æ—¥æœŸæ•°: {migrated_count}")
    print(f"   - ç´¢å¼•æ–‡ä»¶: {INDEX_FILE}")
    print(f"   - æ—¥å¿—ç›®å½•: {CACHE_DIR / 'logs/'}")
    print("=" * 60)
    
    # å¤‡ä»½æ—§ç¼“å­˜
    backup_file = OLD_CACHE_FILE.with_suffix('.json.backup')
    old_cache.rename(backup_file)
    print(f"ğŸ’¾ æ—§ç¼“å­˜å·²å¤‡ä»½åˆ°: {backup_file}")

if __name__ == "__main__":
    main()
